# ğŸ¥ Medical Chatbot â€“ AI-Powered Health Assistance

An AI-driven medical chatbot built using **FastAPI (backend)** and **React (frontend)**.  
It provides users with symptom-based responses, medical information retrieval, and general health guidance.  
This project is designed for learning, experimentation, and demonstrating end-to-end AI application development.

---

## ğŸš€ Features

- ğŸ¤– **AI Chatbot** for answering basic medical queries  
- ğŸ“š **Knowledge Retrieval / RAG** support (optional)  
- âš¡ **FastAPI backend** with async endpoints  
- ğŸŒ **React frontend** for a clean chat UI  
- ğŸ” **CORS enabled** for secure clientâ€“server communication  
- ğŸ“ **Location-based services** (optional: geopy, Google Maps API)  
- ğŸ§ª **ML model integration** (optional: sklearn, joblib, MLflow)

---

## ğŸ›  Tech Stack

### **Backend**
- Python 3.10+
- FastAPI
- Uvicorn
- Pydantic
- geopy (optional)
- httpx (for API calls)
- MLflow / joblib (optional for ML models)

### **Frontend**
- React + Vite (or CRA)
- TailwindCSS (optional)
- Axios for API communication

---

## ğŸ“ Project Structure

Medical_chatbot/

â”œâ”€â”€ backend/

â”‚     â”œâ”€â”€ main.py

â”‚     â”œâ”€â”€ requirements.txt

â”‚     â”œâ”€â”€ app.py

â”‚     â”œâ”€â”€ ingest.py

â”‚     â”œâ”€â”€ rag_chain.py

â”‚

â”œâ”€â”€ frontend/

â”‚     â”œâ”€â”€dist/

â”‚     â”œâ”€â”€ src/

â”‚     â”œâ”€â”€ public/

â”‚     â””â”€â”€ package.json

â”‚

â””â”€â”€ README.md


---

## âš™ï¸ Installation & Setup

### ğŸ“Œ **1. Clone the Repository**
```bash
git clone https://github.com/Dinesh-Sharma2004/Medical_chatbot.git
cd Medical_Chatbot
```

### ğŸ“Œ 2. Backend Setup (FastAPI)**
```bash
Copy code
cd backend
python -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```
Run the server:

```bash
Copy code
uvicorn main:app --host 0.0.0.0 --port 8000
```
### ğŸ“Œ 3. Frontend Setup (React)**
```bash
Copy code
cd frontend
npm install
npm run dev
```
ğŸ”§ Environment Variables
Create a .env file inside backend/:

ini

Copy code

RETRIEVER_K=8

RETRIEVER_FETCH_K=20

RETRIEVER_RERANK_TOP_K=5

USE_RERANKER=false

LLM_MAX_TOKENS=300

LLM_DEVICE=auto

LLM_QUANTIZE=false

LLM_TEMPERATURE=0.1

GROQ_API_KEYS="your-api-keys(I used 15 with comma separated values in one line)"

GROQ_MODEL=llama-3.1-8b-instant

EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2

EMBED_BATCH_SIZE=32

DB_FAISS_BASE="vectorstore"

HF_HOME="/cache/huggingface"

HF_HUB_CACHE="/cache/huggingface"

### ğŸ“¡ API Endpoints**

Method	Endpoint	Description
ğŸš€ 1. Health Check
GET /api/health

Returns the live status of the backend, including vector store & LLM readiness.

Response
```bash
{
  "status": "ok",
  "vector_ready": true,
  "llm_ready": true,
  "detail": {
    "vectorstore": true,
    "llm": true
  }
}
```
ğŸ“„ 2. PDF Upload & Ingestion
POST /api/upload

Uploads a PDF file, saves it, and triggers background ingestion â†’ embeddings â†’ FAISS vector store creation.

Form-Data
Field	Type	Description
file	file (.pdf)	PDF document to ingest
```
Response
{
  "ok": true,
  "job_id": "uuid",
  "filename": "uploaded.pdf"
}
```
GET /api/upload/status/{job_id}

Fetches the ongoing ingestion progress.

```Response Example

{
  "job_id": "1234",
  "filename": "report.pdf",
  "status": "processing",
  "progress": 60,
  "detail": "Chunking pages"
}
```

Status values:
```
processing

completed

error
```
ğŸ“š 3. Full Document Text Retrieval
GET /api/source/{doc_id}

Returns the fulltext representation of a document chunk created during ingestion.

Response
```
{
  "doc_id": "page_3",
  "text": "Full page extracted text..."
}
```
ğŸ’¬ 4. Non-Streaming Question Answering
POST /api/ask

Sends a user query and returns the final answer using RAG (Groq + FAISS).

Form Fields
Field	Type	Default	Description
question	string	required	User query
mode	string	"basic"	RAG chain mode

```
Response
{
  "answer": "The answer...",
  "sources": [
    {
      "filename": "file.pdf",
      "page": 12,
      "doc_id": "chunk_12"
    }
  ],
  "mode": "basic"
}
```

ğŸ”„ 5. Streaming Question Answering (NDJSON)
POST /api/ask/stream

Returns the answer token-by-token (Groq streaming). Uses NDJSON format where every line is a JSON object.

```
Body (JSON)
{
  "question": "What is diabetes?",
  "mode": "basic"
}
```
Streaming Event Types
Type	Meaning
sources	First event â†’ List of retrieved chunks
partial	Partial answer chunk
done	Completion signal
error	Error message
Example Stream
```
{"type":"sources","sources":[...]}
{"type":"partial","text":"Diabetes is..."}
{"type":"partial","text":"a metabolic disorder..."}
{"type":"done"}
```

ğŸ–¥ï¸ 6. Frontend Serving (Vite Build)

If the frontend build exists, it serves the SPA:

GET /
```
Serves index.html from Viteâ€™s dist/ folder.
```
ğŸ§ª 7. Frontend Info Endpoint
GET /_frontend_info

Returns the actual location of the frontend distribution folder.

```
Response
{
  "frontend_dist": "/path/to/dist",
  "exists": true
}
```


ğŸ§  How It Works
The user sends a message via the React UI

The message is forwarded to FastAPI

Backend processes the query using:

LLM / RAG

ML model

Custom rule-based logic

Response is returned to frontend

User sees the AI-generated output

ğŸ§ª Running Tests
```bash
Copy code
pytest
ğŸ›¤ Roadmap
 Add vector database (Chroma / FAISS)

 Deploy backend to Fly.io or Render

 Deploy frontend to Vercel

 Add user authentication

 Implement chat history storage
```
ğŸ¤ Contributing
Pull requests are welcome!
For major changes, please open an issue first to discuss your ideas.

ğŸ“„ License
This project is licensed under the MIT License.

â­ Support
If you find this project helpful, please star the repository â­
